{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtzig/LIDC_GDRO/blob/main/notebooks/lidc_cnn_ERM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXP5BUZY7SbH"
      },
      "source": [
        "#ERM CNN Model for Malignancy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First We setup the repo"
      ],
      "metadata": {
        "id": "3mlU0-uy8Ez3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMqJTbcB7SbG",
        "outputId": "58f6ceb3-a2de-4d60-f7a3-c1c0223fed71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LIDC_GDRO'...\n",
            "remote: Enumerating objects: 3070, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/145)\u001b[K\rremote: Counting objects:   1% (2/145)\u001b[K\rremote: Counting objects:   2% (3/145)\u001b[K\rremote: Counting objects:   3% (5/145)\u001b[K\rremote: Counting objects:   4% (6/145)\u001b[K\rremote: Counting objects:   5% (8/145)\u001b[K\rremote: Counting objects:   6% (9/145)\u001b[K\rremote: Counting objects:   7% (11/145)\u001b[K\rremote: Counting objects:   8% (12/145)\u001b[K\rremote: Counting objects:   9% (14/145)\u001b[K\rremote: Counting objects:  10% (15/145)\u001b[K\rremote: Counting objects:  11% (16/145)\u001b[K\rremote: Counting objects:  12% (18/145)\u001b[K\rremote: Counting objects:  13% (19/145)\u001b[K\rremote: Counting objects:  14% (21/145)\u001b[K\rremote: Counting objects:  15% (22/145)\u001b[K\rremote: Counting objects:  16% (24/145)\u001b[K\rremote: Counting objects:  17% (25/145)\u001b[K\rremote: Counting objects:  18% (27/145)\u001b[K\rremote: Counting objects:  19% (28/145)\u001b[K\rremote: Counting objects:  20% (29/145)\u001b[K\rremote: Counting objects:  21% (31/145)\u001b[K\rremote: Counting objects:  22% (32/145)\u001b[K\rremote: Counting objects:  23% (34/145)\u001b[K\rremote: Counting objects:  24% (35/145)\u001b[K\rremote: Counting objects:  25% (37/145)\u001b[K\rremote: Counting objects:  26% (38/145)\u001b[K\rremote: Counting objects:  27% (40/145)\u001b[K\rremote: Counting objects:  28% (41/145)\u001b[K\rremote: Counting objects:  29% (43/145)\u001b[K\rremote: Counting objects:  30% (44/145)\u001b[K\rremote: Counting objects:  31% (45/145)\u001b[K\rremote: Counting objects:  32% (47/145)\u001b[K\rremote: Counting objects:  33% (48/145)\u001b[K\rremote: Counting objects:  34% (50/145)\u001b[K\rremote: Counting objects:  35% (51/145)\u001b[K\rremote: Counting objects:  36% (53/145)\u001b[K\rremote: Counting objects:  37% (54/145)\u001b[K\rremote: Counting objects:  38% (56/145)\u001b[K\rremote: Counting objects:  39% (57/145)\u001b[K\rremote: Counting objects:  40% (58/145)\u001b[K\rremote: Counting objects:  41% (60/145)\u001b[K\rremote: Counting objects:  42% (61/145)\u001b[K\rremote: Counting objects:  43% (63/145)\u001b[K\rremote: Counting objects:  44% (64/145)\u001b[K\rremote: Counting objects:  45% (66/145)\u001b[K\rremote: Counting objects:  46% (67/145)\u001b[K\rremote: Counting objects:  47% (69/145)\u001b[K\rremote: Counting objects:  48% (70/145)\u001b[K\rremote: Counting objects:  49% (72/145)\u001b[K\rremote: Counting objects:  50% (73/145)\u001b[K\rremote: Counting objects:  51% (74/145)\u001b[K\rremote: Counting objects:  52% (76/145)\u001b[K\rremote: Counting objects:  53% (77/145)\u001b[K\rremote: Counting objects:  54% (79/145)\u001b[K\rremote: Counting objects:  55% (80/145)\u001b[K\rremote: Counting objects:  56% (82/145)\u001b[K\rremote: Counting objects:  57% (83/145)\u001b[K\rremote: Counting objects:  58% (85/145)\u001b[K\rremote: Counting objects:  59% (86/145)\u001b[K\rremote: Counting objects:  60% (87/145)\u001b[K\rremote: Counting objects:  61% (89/145)\u001b[K\rremote: Counting objects:  62% (90/145)\u001b[K\rremote: Counting objects:  63% (92/145)\u001b[K\rremote: Counting objects:  64% (93/145)\u001b[K\rremote: Counting objects:  65% (95/145)\u001b[K\rremote: Counting objects:  66% (96/145)\u001b[K\rremote: Counting objects:  67% (98/145)\u001b[K\rremote: Counting objects:  68% (99/145)\u001b[K\rremote: Counting objects:  69% (101/145)\u001b[K\rremote: Counting objects:  70% (102/145)\u001b[K\rremote: Counting objects:  71% (103/145)\u001b[K\rremote: Counting objects:  72% (105/145)\u001b[K\rremote: Counting objects:  73% (106/145)\u001b[K\rremote: Counting objects:  74% (108/145)\u001b[K\rremote: Counting objects:  75% (109/145)\u001b[K\rremote: Counting objects:  76% (111/145)\u001b[K\rremote: Counting objects:  77% (112/145)\u001b[K\rremote: Counting objects:  78% (114/145)\u001b[K\rremote: Counting objects:  79% (115/145)\u001b[K\rremote: Counting objects:  80% (116/145)\u001b[K\rremote: Counting objects:  81% (118/145)\u001b[K\rremote: Counting objects:  82% (119/145)\u001b[K\rremote: Counting objects:  83% (121/145)\u001b[K\rremote: Counting objects:  84% (122/145)\u001b[K\rremote: Counting objects:  85% (124/145)\u001b[K\rremote: Counting objects:  86% (125/145)\u001b[K\rremote: Counting objects:  87% (127/145)\u001b[K\rremote: Counting objects:  88% (128/145)\u001b[K\rremote: Counting objects:  89% (130/145)\u001b[K\rremote: Counting objects:  90% (131/145)\u001b[K\rremote: Counting objects:  91% (132/145)\u001b[K\rremote: Counting objects:  92% (134/145)\u001b[K\rremote: Counting objects:  93% (135/145)\u001b[K\rremote: Counting objects:  94% (137/145)\u001b[K\rremote: Counting objects:  95% (138/145)\u001b[K\rremote: Counting objects:  96% (140/145)\u001b[K\rremote: Counting objects:  97% (141/145)\u001b[K\rremote: Counting objects:  98% (143/145)\u001b[K\rremote: Counting objects:  99% (144/145)\u001b[K\rremote: Counting objects: 100% (145/145)\u001b[K\rremote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 3070 (delta 79), reused 89 (delta 38), pack-reused 2925\u001b[K\n",
            "Receiving objects: 100% (3070/3070), 39.80 MiB | 33.40 MiB/s, done.\n",
            "Resolving deltas: 100% (2891/2891), done.\n",
            "Checking out files: 100% (5386/5386), done.\n",
            "/content/LIDC_GDRO\n"
          ]
        }
      ],
      "source": [
        "# Only run if on Colab\n",
        "#%cd .. #run this on local machine\n",
        "\n",
        "\n",
        "!git clone https://github.com/mtzig/LIDC_GDRO.git\n",
        "%cd /content/LIDC_GDRO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "FfxoU2RZ7SbI"
      },
      "outputs": [],
      "source": [
        "#!git pull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "BoTdOzRN7hbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31810087-8d58-4760-ce0e-cd3aecbb31f2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dataloaders import InfiniteDataLoader\n",
        "from datasets import NoduleDataset\n",
        "from models import VGGNet, ResNet18\n",
        "from loss import ERMLoss\n",
        "from train import train"
      ],
      "metadata": {
        "id": "Gor5x5428B0V"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Good to go!\")\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Using cpu\")\n",
        "    DEVICE = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6Vy4YCE8UHA",
        "outputId": "d6e7bcc6-c881-430e-ca4e-6854b2a8ec43"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next We get our data"
      ],
      "metadata": {
        "id": "XHCAFpSD8WpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First some functions to retrive the data"
      ],
      "metadata": {
        "id": "62LXFK8v-vdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getNormed(this_array, this_min = 0, this_max = 255, set_to_int = True):\n",
        "    \n",
        "    rat = (this_max - this_min)/(this_array.max() - this_array.min())\n",
        "    this_array = this_array * rat\n",
        "    this_array -= this_array.min()\n",
        "    this_array += this_min\n",
        "    if set_to_int:\n",
        "        return this_array.to(dtype= torch.int)\n",
        "    return this_array"
      ],
      "metadata": {
        "id": "XHRheK019LX8"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getImages(image_folder):\n",
        "    '''\n",
        "        Input:\n",
        "        image_folder: directory of the image files\n",
        "\n",
        "        Output:\n",
        "        m1: list of the labels encountered (1,2,4,5)\n",
        "        m2: list of binary labels encountered (benign, malignant)\n",
        "        diff: list of any nodes with discrepency to CSV labels\n",
        "\n",
        "    '''\n",
        "    \n",
        "    train_img = []\n",
        "    train_label = []\n",
        "    train_spic_label = []\n",
        "    marked_benign = []\n",
        "    unmarked_benign = []\n",
        "    \n",
        "    marked_malignant = []\n",
        "    unmarked_malignant = []\n",
        "\n",
        "    \n",
        "\n",
        "    lidc = pd.read_csv('./data/lidc_spic_subgrouped.csv')\n",
        "    train_test = pd.read_csv('./data/lidc_train_test_split_stratified.csv')\n",
        "    for dir1 in os.listdir(image_folder):\n",
        "  \n",
        "        if dir1 == 'Malignancy_3':\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(os.path.join(image_folder, dir1)):\n",
        "\n",
        "\n",
        "            temp_nodule_ID = file.split('.')[0]\n",
        "            subtype = lidc[lidc['noduleID']==int(temp_nodule_ID)]['subgroup'].iloc[0]\n",
        "            malignancy = lidc[lidc['noduleID']==int(temp_nodule_ID)]['malignancy'].iloc[0]\n",
        "            spiculation = lidc[lidc['noduleID']==int(temp_nodule_ID)]['malignancy'].iloc[0]\n",
        "            \n",
        "            train_type = train_test[train_test['noduleID'] ==int(temp_nodule_ID)]['dataset'].iloc[0]\n",
        "            \n",
        "            \n",
        "            image = np.loadtxt(os.path.join(image_folder, dir1,file))\n",
        "            image = torch.from_numpy(image).to(DEVICE)\n",
        "            rgb_image = torch.stack((image,image,image), dim = 0)\n",
        "            rgb_image = getNormed(rgb_image)\n",
        "            rgb_image = rgb_image / 255 \n",
        "\n",
        "            \n",
        "            \n",
        "            if train_type == 'train':\n",
        "                train_img.append(rgb_image)\n",
        "                train_label.append(torch.tensor(1).to(DEVICE).to(torch.float32) if malignancy > 3 else torch.tensor(0).to(DEVICE).to(torch.float32))\n",
        "                train_spic_label.append(torch.tensor(1).to(DEVICE).to(torch.float32) if spiculation > 1 else torch.tensor(0).to(DEVICE).to(torch.float32))\n",
        "                \n",
        "                continue\n",
        "            \n",
        "            if subtype == 'marked_benign':\n",
        "                image_array = marked_benign\n",
        "            elif subtype == 'unmarked_benign':\n",
        "                image_array = unmarked_benign\n",
        "            elif subtype == 'marked_malignant':\n",
        "                image_array = marked_malignant\n",
        "            else:\n",
        "                image_array = unmarked_malignant\n",
        "            \n",
        "            image_array.append(rgb_image)\n",
        " \n",
        "\n",
        "\n",
        "    return train_img, train_label, train_spic_label, marked_benign, unmarked_benign, marked_malignant, unmarked_malignant"
      ],
      "metadata": {
        "id": "TQF8Ig1z-ld5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we get the data"
      ],
      "metadata": {
        "id": "znDlzTCe-1AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_img, train_label, train_spic_label, marked_benign, unmarked_benign, marked_malignant, unmarked_malignant = getImages('./LIDC(MaxSlices)_Nodules(fixed)')"
      ],
      "metadata": {
        "id": "vq6T0fX4-6FA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NoduleDataset(train_img, train_label)"
      ],
      "metadata": {
        "id": "Pi25v4H0-7d4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "id": "rdJk4t5E_D06",
        "outputId": "0de41a8f-e21f-4dca-e950-1921cbbf1138",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1210"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [1000, 210])\n",
        "\n",
        "train_loader = InfiniteDataLoader(train_set, 1000)\n",
        "val_loader = InfiniteDataLoader(val_set, len(val_set))"
      ],
      "metadata": {
        "id": "0pRQhnBJ_EUJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now we create the model and setup training"
      ],
      "metadata": {
        "id": "zxegLjbZ5Dex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we make our model"
      ],
      "metadata": {
        "id": "bScAw_b25T1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet18(device=DEVICE)"
      ],
      "metadata": {
        "id": "iAG-NHBe5KE6"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = ERMLoss(model,torch.nn.functional.binary_cross_entropy_with_logits,{})\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.005)\n"
      ],
      "metadata": {
        "id": "RmxOUGwp5WDm"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now we train the model"
      ],
      "metadata": {
        "id": "jHLB-GyZ7M2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    train(train_loader, model, loss_fn, optimizer, verbose=True)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      X,y = next(val_loader)\n",
        "      results = torch.sigmoid(model(X))\n",
        "      accuracy = torch.sum(torch.round(results) == y)/X.shape[0]\n",
        "    model.train()\n",
        "    print(f'cv accuracy {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEkXaPy15b4c",
        "outputId": "f64b636f-5ba6-46eb-a6be-3598bc9f72c3"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "Average training loss: 0.7078949213027954\n",
            "cv accuracy 0.5142857432365417\n",
            "Epoch 2/40\n",
            "Average training loss: 0.5792198777198792\n",
            "cv accuracy 0.5761904716491699\n",
            "Epoch 3/40\n",
            "Average training loss: 0.5291832685470581\n",
            "cv accuracy 0.5857142806053162\n",
            "Epoch 4/40\n",
            "Average training loss: 0.4468575716018677\n",
            "cv accuracy 0.6333333849906921\n",
            "Epoch 5/40\n",
            "Average training loss: 0.40774005651474\n",
            "cv accuracy 0.6571428775787354\n",
            "Epoch 6/40\n",
            "Average training loss: 0.3279047906398773\n",
            "cv accuracy 0.6809524297714233\n",
            "Epoch 7/40\n",
            "Average training loss: 0.283476322889328\n",
            "cv accuracy 0.6809524297714233\n",
            "Epoch 8/40\n",
            "Average training loss: 0.21565327048301697\n",
            "cv accuracy 0.6904762387275696\n",
            "Epoch 9/40\n",
            "Average training loss: 0.1593194305896759\n",
            "cv accuracy 0.6619048118591309\n",
            "Epoch 10/40\n",
            "Average training loss: 0.13348686695098877\n",
            "cv accuracy 0.6857143044471741\n",
            "Epoch 11/40\n",
            "Average training loss: 0.09123595058917999\n",
            "cv accuracy 0.7142857313156128\n",
            "Epoch 12/40\n",
            "Average training loss: 0.06538736075162888\n",
            "cv accuracy 0.6571428775787354\n",
            "Epoch 13/40\n",
            "Average training loss: 0.051394425332546234\n",
            "cv accuracy 0.7047619223594666\n",
            "Epoch 14/40\n",
            "Average training loss: 0.03266988322138786\n",
            "cv accuracy 0.6666666865348816\n",
            "Epoch 15/40\n",
            "Average training loss: 0.02710885740816593\n",
            "cv accuracy 0.6904762387275696\n",
            "Epoch 16/40\n",
            "Average training loss: 0.021990593522787094\n",
            "cv accuracy 0.6666666865348816\n",
            "Epoch 17/40\n",
            "Average training loss: 0.01727515459060669\n",
            "cv accuracy 0.6476190686225891\n",
            "Epoch 18/40\n",
            "Average training loss: 0.014863630756735802\n",
            "cv accuracy 0.6285714507102966\n",
            "Epoch 19/40\n",
            "Average training loss: 0.012633133679628372\n",
            "cv accuracy 0.7190476655960083\n",
            "Epoch 20/40\n",
            "Average training loss: 0.010600979439914227\n",
            "cv accuracy 0.6714286208152771\n",
            "Epoch 21/40\n",
            "Average training loss: 0.010561724193394184\n",
            "cv accuracy 0.6809524297714233\n",
            "Epoch 22/40\n",
            "Average training loss: 0.007536319084465504\n",
            "cv accuracy 0.7190476655960083\n",
            "Epoch 23/40\n",
            "Average training loss: 0.006256651598960161\n",
            "cv accuracy 0.7523809671401978\n",
            "Epoch 24/40\n",
            "Average training loss: 0.005194270517677069\n",
            "cv accuracy 0.7000000476837158\n",
            "Epoch 25/40\n",
            "Average training loss: 0.0055298092775046825\n",
            "cv accuracy 0.7333333492279053\n",
            "Epoch 26/40\n",
            "Average training loss: 0.006087240297347307\n",
            "cv accuracy 0.7047619223594666\n",
            "Epoch 27/40\n",
            "Average training loss: 0.004493757616728544\n",
            "cv accuracy 0.6666666865348816\n",
            "Epoch 28/40\n",
            "Average training loss: 0.0037171870935708284\n",
            "cv accuracy 0.7428571581840515\n",
            "Epoch 29/40\n",
            "Average training loss: 0.00357089564204216\n",
            "cv accuracy 0.7428571581840515\n",
            "Epoch 30/40\n",
            "Average training loss: 0.004377332516014576\n",
            "cv accuracy 0.7047619223594666\n",
            "Epoch 31/40\n",
            "Average training loss: 0.003658880013972521\n",
            "cv accuracy 0.7285714745521545\n",
            "Epoch 32/40\n",
            "Average training loss: 0.002662048675119877\n",
            "cv accuracy 0.6809524297714233\n",
            "Epoch 33/40\n",
            "Average training loss: 0.0027095687109977007\n",
            "cv accuracy 0.6809524297714233\n",
            "Epoch 34/40\n",
            "Average training loss: 0.00373916607350111\n",
            "cv accuracy 0.7190476655960083\n",
            "Epoch 35/40\n",
            "Average training loss: 0.0030256379395723343\n",
            "cv accuracy 0.723809540271759\n",
            "Epoch 36/40\n",
            "Average training loss: 0.0021023780573159456\n",
            "cv accuracy 0.6904762387275696\n",
            "Epoch 37/40\n",
            "Average training loss: 0.002533126622438431\n",
            "cv accuracy 0.6571428775787354\n",
            "Epoch 38/40\n",
            "Average training loss: 0.0021172405686229467\n",
            "cv accuracy 0.7952381372451782\n",
            "Epoch 39/40\n",
            "Average training loss: 0.0021077238488942385\n",
            "cv accuracy 0.7142857313156128\n",
            "Epoch 40/40\n",
            "Average training loss: 0.0029911561869084835\n",
            "cv accuracy 0.747619092464447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lastly We evaluate model performance"
      ],
      "metadata": {
        "id": "ZYpAh47-7Qb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first create a simple function to get sensitivities"
      ],
      "metadata": {
        "id": "q2O87Mt_7W_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sensitivity(model, imgs, label, label_tensor = False):\n",
        "  '''\n",
        "  Inputs:\n",
        "  model: the model to use\n",
        "  img: list of imgs in the class\n",
        "  label: either 0 or 1 depending on the ground truth of subclass\n",
        "  label_tensor: if True, then label is tensor of ground truth\n",
        "\n",
        "  Output:\n",
        "  accuracy: accuracy for this subgroup\n",
        "\n",
        "  '''\n",
        "  results = torch.sigmoid(model(torch.stack(imgs).to(DEVICE)))\n",
        "  if label_tensor:\n",
        "    truth = label\n",
        "  elif label == 1:\n",
        "    truth = torch.ones(len(imgs), device=DEVICE)\n",
        "  else:\n",
        "    truth = torch.zeros(len(imgs), device=DEVICE)\n",
        "  \n",
        "  accuracy = torch.sum(torch.round(results) == truth)/len(imgs)\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "lWp3Z6kH7L0a"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Performance on Test Set"
      ],
      "metadata": {
        "id": "jqL4pnBH8gG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spaghetti code-esque way to get imgs and labels for entire test set\n",
        "all_test_imgs = marked_benign+unmarked_benign+marked_malignant+unmarked_malignant\n",
        "all_labels = torch.tensor([0 for _ in marked_benign+unmarked_benign]+[1 for _ in marked_malignant+unmarked_malignant], device=DEVICE)\n",
        "\n",
        "\n",
        "print(f'spiculated benign accuracy: {get_sensitivity(model, marked_benign, 0):.3f}')\n",
        "print(f'unspiculated benign accuracy: {get_sensitivity(model, unmarked_benign, 0):.3f}')\n",
        "print(f'spiculated malignant accuracy: {get_sensitivity(model, marked_malignant, 1):.3f}')\n",
        "print(f'unspiculated malignant accuracy: {get_sensitivity(model, unmarked_malignant, 1):.3f}')\n",
        "\n",
        "print(f'Total accuracy: {get_sensitivity(model, all_test_imgs, all_labels, label_tensor=True):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLmZBVdP8b1s",
        "outputId": "4b4de0e3-e8d7-4c46-fb98-07f7a826679a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spiculated benign accuracy: 0.762\n",
            "unspiculated benign accuracy: 0.572\n",
            "spiculated malignant accuracy: 0.512\n",
            "unspiculated malignant accuracy: 0.491\n",
            "Total accuracy: 0.757\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "lidc_cnn_ERM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}