{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MsAsterisk1/ERM_GDRO/blob/main/civil_comments_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UJn54eo6aTIC"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCeDv917mrC6",
    "outputId": "6206725f-8228-4122-83c8-545c99c18f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2060 (UUID: GPU-100f20af-1b36-5adc-0795-f06b020c050b)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1pwy_5DbGZgk",
    "outputId": "63ae86f4-a027-4edb-ded2-c241bd8d3f33"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/MsAsterisk1/ERM_GDRO.git\n",
    "# %cd /content/ERM_GDRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QE2ClDuV-7mx"
   },
   "outputs": [],
   "source": [
    "# !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZ8NiHj3dQim",
    "outputId": "848d4fd5-b2de-4a71-829e-f3d9c6796a42",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NsA17YlmbQ-8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.process_data_utils import  get_CivilComments_DataLoaders, get_CivilComments_Datasets, get_CivilComments_df\n",
    "from loss import ERMLoss, GDROLoss, ERMGDROLoss, UpweightLoss\n",
    "from train_eval import train, evaluate, train_epochs\n",
    "import torch\n",
    "from models import BertClassifier\n",
    "from math import ceil\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUx_MfAZdOtB"
   },
   "source": [
    "Now lets try getting Bert working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tiSrSPzdBN9q",
    "outputId": "b4ff6b24-94dc-4e74-a206-7846e8584317"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cPlJw_w0Jclx"
   },
   "outputs": [],
   "source": [
    "df = get_CivilComments_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K3_JuPgTDPg-"
   },
   "outputs": [],
   "source": [
    "datasets = get_CivilComments_Datasets(CC_df=df, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7_G3XT5g_OPa"
   },
   "outputs": [],
   "source": [
    "tr,cv,test = get_CivilComments_DataLoaders(datasets=datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7n857zmGfiK2"
   },
   "source": [
    "This is how we would initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxdghEABLCrn",
    "outputId": "5c6a7842-b1b2-4890-803f-8039fe83f582"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n",
      "Average training loss: 0.2264478290124004\n",
      "For Cross Val:\n",
      "Loss: 0.22557975351810455 Accuracy: 0.9049358123063302 \n",
      "Accuracy over subgroups: [0.60699301 0.5849546  0.55865922 0.47916667 0.54296875 0.2\n",
      " 0.67354597 0.62676056 0.68678029 0.92444444 0.93417969 0.83894449\n",
      " 0.9668747  0.88861076 0.92592593 0.77479893 0.79950372 0.95365328] \n",
      "Worst Group Accuracy: 0.2\n",
      "Epoch 2 / 5\n",
      "Average training loss: 0.21623695677513527\n",
      "For Cross Val:\n",
      "Loss: 0.23032869398593903 Accuracy: 0.8903718459495352 \n",
      "Accuracy over subgroups: [0.7020979  0.65888457 0.70949721 0.57291667 0.66992188 0.6\n",
      " 0.79924953 0.77699531 0.72409028 0.88987654 0.90390625 0.70973612\n",
      " 0.94959193 0.80913642 0.92592593 0.63538874 0.64813896 0.94201763] \n",
      "Worst Group Accuracy: 0.5729166666666666\n",
      "Epoch 3 / 5\n",
      "Average training loss: 0.2140838625643944\n",
      "For Cross Val:\n",
      "Loss: 0.20929887890815735 Accuracy: 0.9173085436033643 \n",
      "Accuracy over subgroups: [0.4027972  0.40726329 0.34636872 0.26041667 0.33398438 0.\n",
      " 0.37898687 0.39319249 0.50391525 0.96938272 0.97265625 0.9399454\n",
      " 0.99039846 0.9649562  1.         0.9285076  0.93151365 0.98385896] \n",
      "Worst Group Accuracy: 0.0\n",
      "Epoch 4 / 5\n",
      "Average training loss: 0.2086389120977229\n",
      "For Cross Val:\n",
      "Loss: 0.20445458590984344 Accuracy: 0.9171978751660027 \n",
      "Accuracy over subgroups: [0.43496503 0.44357977 0.40782123 0.28645833 0.36132812 0.\n",
      " 0.46716698 0.44131455 0.52372179 0.9617284  0.96660156 0.92083712\n",
      " 0.98799808 0.95306633 0.96296296 0.8945487  0.90918114 0.98205681] \n",
      "Worst Group Accuracy: 0.0\n",
      "Epoch 5 / 5\n",
      "Average training loss: 0.2050100893639593\n",
      "For Cross Val:\n",
      "Loss: 0.20334970951080322 Accuracy: 0.9171314741035856 \n",
      "Accuracy over subgroups: [0.51188811 0.50194553 0.46648045 0.32552083 0.41601562 0.\n",
      " 0.55534709 0.51408451 0.57438968 0.95481481 0.95546875 0.89080983\n",
      " 0.9824772  0.94242804 0.96296296 0.86595174 0.88387097 0.97712047] \n",
      "Worst Group Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "epochs = 5\n",
    "loss_fn = ERMLoss(model, torch.nn.CrossEntropyLoss())\n",
    "# loss_fn = GDROLoss(model, torch.nn.CrossEntropyLoss(), 0.01, 8, vector_subclass=True)\n",
    "\n",
    "train_epochs(epochs, tr, cv, test, model, loss_fn, optimizer, vector_subclass=True, verbose=True, num_subclasses=18, save_weights_name='CC_ERM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of utils.process_data_utils failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tzeng1\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\extensions\\autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\tzeng1\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Program Files\\Python310\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\tzeng1\\repos\\ERM_GDRO\\utils\\process_data_utils.py\", line 5, in <module>\n",
      "    from wilds import get_dataset\n",
      "ModuleNotFoundError: No module named 'wilds'\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9160200923891106 \n",
      "Accuracy over subgroups: [0.52065365 0.50881057 0.53700658 0.4047619  0.43577136 0.21428571\n",
      " 0.56668835 0.53294746 0.57015825 0.94748594 0.95711968 0.87133956\n",
      " 0.97686142 0.92810458 0.98019802 0.85007496 0.88956841 0.97650703] \n",
      "Worst Group Accuracy: 0.21428571428571427\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate(test, model, 18, vector_subclass=True, replacement=False, get_loss=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8887742745660852 \n",
      "Accuracy over subgroups: [0.6895143  0.66035242 0.77878289 0.59047619 0.67301782 0.42857143\n",
      " 0.80351334 0.79608192 0.71650339 0.88256699 0.90330771 0.68224299\n",
      " 0.93438559 0.79402428 0.93069307 0.63688156 0.65717281 0.94212067] \n",
      "Worst Group Accuracy: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./epoch_2_CC_ERM.wt'))\n",
    "_ = evaluate(test, model, 18, vector_subclass=True, replacement=False, get_loss=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.917021721905787 \n",
      "Accuracy over subgroups: [0.43758511 0.42687225 0.40296053 0.32777778 0.34357714 0.14285714\n",
      " 0.41314249 0.42742654 0.50293896 0.96543169 0.97355244 0.9211838\n",
      " 0.98545575 0.95966387 0.99009901 0.92413793 0.93377599 0.98365992] \n",
      "Worst Group Accuracy: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./epoch_3_CC_ERM.wt'))\n",
    "_ = evaluate(test, model, 18, vector_subclass=True, replacement=False, get_loss=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n",
      "Average training loss: 0.26088099510043666\n",
      "For Cross Val:\n",
      "Loss: 0.2877463400363922 Accuracy: 0.8711376715360779 \n",
      "Accuracy over subgroups: [0.65594406 0.67574578 0.4301676  0.56510417 0.50390625 1.\n",
      " 0.4727955  0.55868545 0.80239521 0.87259259 0.86308594 0.84804368\n",
      " 0.92870859 0.84230288 0.85185185 0.83467382 0.82084367 0.90695397] \n",
      "Worst Group Accuracy: 0.4301675977653631\n",
      "Epoch 2 / 5\n",
      "Average training loss: 0.23896933662595182\n",
      "For Cross Val:\n",
      "Loss: 0.26259908080101013 Accuracy: 0.8866533864541832 \n",
      "Accuracy over subgroups: [0.60559441 0.61089494 0.39106145 0.515625   0.46289062 0.6\n",
      " 0.44652908 0.5258216  0.78765546 0.90395062 0.90742188 0.90172884\n",
      " 0.95055209 0.89486859 0.92592593 0.87578195 0.85508685 0.9212145 ] \n",
      "Worst Group Accuracy: 0.39106145251396646\n",
      "Epoch 3 / 5\n",
      "Average training loss: 0.23078708476787133\n",
      "For Cross Val:\n",
      "Loss: 0.2884288430213928 Accuracy: 0.8688800354138999 \n",
      "Accuracy over subgroups: [0.66713287 0.67444877 0.46648045 0.56770833 0.546875   0.6\n",
      " 0.51594747 0.60211268 0.82772916 0.87703704 0.87675781 0.85987261\n",
      " 0.93566971 0.8485607  0.88888889 0.82126899 0.8044665  0.897238  ] \n",
      "Worst Group Accuracy: 0.4664804469273743\n",
      "Epoch 4 / 5\n",
      "Average training loss: 0.22202276298133786\n",
      "For Cross Val:\n",
      "Loss: 0.3477458655834198 Accuracy: 0.8453297919433378 \n",
      "Accuracy over subgroups: [0.72447552 0.73022049 0.53072626 0.6484375  0.6015625  1.\n",
      " 0.5891182  0.65962441 0.86872409 0.83703704 0.83886719 0.80163785\n",
      " 0.90326452 0.79036295 0.88888889 0.77211796 0.74937965 0.86856024] \n",
      "Worst Group Accuracy: 0.5307262569832403\n",
      "Epoch 5 / 5\n",
      "Average training loss: 0.216076873391983\n",
      "For Cross Val:\n",
      "Loss: 0.35902509093284607 Accuracy: 0.833244798583444 \n",
      "Accuracy over subgroups: [0.73146853 0.74837873 0.58938547 0.67447917 0.66210938 1.\n",
      " 0.60787992 0.70070423 0.88392446 0.82419753 0.83417969 0.79526843\n",
      " 0.89774364 0.74217772 0.88888889 0.74977659 0.7057072  0.85230167] \n",
      "Worst Group Accuracy: 0.5893854748603352\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "epochs = 5\n",
    "# loss_fn = ERMLoss(model, torch.nn.CrossEntropyLoss())\n",
    "loss_fn = GDROLoss(model, torch.nn.CrossEntropyLoss(), 0.01, 18, vector_subclass=True)\n",
    "\n",
    "train_epochs(epochs, tr, cv, test, model, loss_fn, optimizer, vector_subclass=True, verbose=True, num_subclasses=18, save_weights_name='CC_GDRO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8308068350002242 \n",
      "Accuracy over subgroups: [0.73990014 0.74977974 0.640625   0.69047619 0.68162262 0.64285714\n",
      " 0.63370202 0.73552983 0.87882442 0.80830301 0.82791452 0.77819315\n",
      " 0.87794397 0.7318394  0.8019802  0.75892054 0.70697187 0.85128555] \n",
      "Worst Group Accuracy: 0.6337020169160703\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('./epoch_2_CC_GDRO.wt'))\n",
    "_ = evaluate(test, model, 18, vector_subclass=True, replacement=False, get_loss=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.843424376971491 \n",
      "Accuracy over subgroups: [0.72991375 0.74185022 0.61184211 0.68650794 0.62999385 0.5\n",
      " 0.60572544 0.68744435 0.86284853 0.8213695  0.82988927 0.79034268\n",
      " 0.88306752 0.77254902 0.79207921 0.77421289 0.74139437 0.86967307] \n",
      "Worst Group Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./epoch_4_CC_GDRO.wt'))\n",
    "_ = evaluate(test, model, 18, vector_subclass=True, replacement=False, get_loss=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n",
      "Average training loss: 0.22609796434781565\n",
      "For Cross Val:\n",
      "Loss: 0.2034807950258255 Accuracy: 0.9156485170429394 \n",
      "Accuracy over subgroups: [0.50909091 0.50194553 0.53631285 0.38802083 0.47851562 0.2\n",
      " 0.59287054 0.5258216  0.57853524 0.95037037 0.95527344 0.85168335\n",
      " 0.98007681 0.91739675 0.96296296 0.81680071 0.86054591 0.97629775] \n",
      "Worst Group Accuracy: 0.2\n",
      "Epoch 2 / 5\n",
      "Average training loss: 0.21557862724064275\n",
      "For Cross Val:\n",
      "Loss: 0.20531503856182098 Accuracy: 0.9164674634794157 \n",
      "Accuracy over subgroups: [0.40839161 0.39429313 0.2849162  0.23697917 0.34179688 0.\n",
      " 0.54409006 0.43779343 0.51727315 0.96888889 0.97285156 0.94358508\n",
      " 0.99111858 0.9649562  1.         0.83288651 0.89627792 0.98272282] \n",
      "Worst Group Accuracy: 0.0\n",
      "Epoch 3 / 5\n",
      "Average training loss: 0.2148581735181079\n",
      "For Cross Val:\n",
      "Loss: 0.20218902826309204 Accuracy: 0.9158698539176627 \n",
      "Accuracy over subgroups: [0.4951049  0.47470817 0.49441341 0.33072917 0.38476562 0.\n",
      " 0.46904315 0.49295775 0.55734684 0.95580247 0.95800781 0.86533212\n",
      " 0.98079693 0.94055069 1.         0.88739946 0.89131514 0.9780999 ] \n",
      "Worst Group Accuracy: 0.0\n",
      "Epoch 4 / 5\n",
      "Average training loss: 0.2091202526375731\n",
      "For Cross Val:\n",
      "Loss: 0.2012563943862915 Accuracy: 0.9163346613545816 \n",
      "Accuracy over subgroups: [0.4979021  0.49027237 0.45251397 0.33333333 0.39453125 0.\n",
      " 0.5272045  0.50469484 0.577614   0.95407407 0.95742187 0.88989991\n",
      " 0.98319731 0.94180225 0.96296296 0.87042002 0.88486352 0.97594515] \n",
      "Worst Group Accuracy: 0.0\n",
      "Epoch 5 / 5\n",
      "Average training loss: 0.20507804133452712\n",
      "For Cross Val:\n",
      "Loss: 0.1985294222831726 Accuracy: 0.9173306772908366 \n",
      "Accuracy over subgroups: [0.49090909 0.47600519 0.45530726 0.3203125  0.3984375  0.\n",
      " 0.51782364 0.50821596 0.54905573 0.95530864 0.959375   0.90263876\n",
      " 0.98391743 0.93992491 0.96296296 0.87131367 0.88883375 0.97986288] \n",
      "Worst Group Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "epochs = 5\n",
    "loss_fn = ERMLoss(model, torch.nn.CrossEntropyLoss())\n",
    "# loss_fn = GDROLoss(model, torch.nn.CrossEntropyLoss(), 0.01, 8, vector_subclass=True)\n",
    "\n",
    "train_epochs(epochs, tr, cv, test, model, loss_fn, optimizer, vector_subclass=True, verbose=True, num_subclasses=18, save_weights_name='CC_ERM_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9172758667085258 \n",
      "Accuracy over subgroups: [0.50022696 0.48634361 0.515625   0.38412698 0.43208359 0.28571429\n",
      " 0.55302537 0.52938557 0.55026375 0.9526133  0.96219762 0.88286604\n",
      " 0.97950583 0.93501401 0.99009901 0.85997001 0.89079154 0.97978817] \n",
      "Worst Group Accuracy: 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('./epoch_5_CC_ERM_2.wt'))\n",
    "_ = evaluate(test, model, 18, vector_subclass=True, replacement=False, get_loss=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n",
      "Average training loss: 0.26006137120775347\n",
      "For Cross Val:\n",
      "Loss: 0.32172897458076477 Accuracy: 0.864032757857459 \n",
      "Accuracy over subgroups: [0.65034965 0.66407263 0.41340782 0.54947917 0.49609375 0.8\n",
      " 0.51219512 0.57746479 0.82726854 0.87135802 0.87460938 0.88353048\n",
      " 0.91886702 0.86545682 0.85185185 0.8230563  0.81042184 0.89159647] \n",
      "Worst Group Accuracy: 0.4134078212290503\n",
      "Epoch 2 / 5\n",
      "Average training loss: 0.2390624364536702\n",
      "For Cross Val:\n",
      "Loss: 0.2904393672943115 Accuracy: 0.8708720672864099 \n",
      "Accuracy over subgroups: [0.63776224 0.67315175 0.41899441 0.56770833 0.484375   0.6\n",
      " 0.46341463 0.55164319 0.8235836  0.87679012 0.87792969 0.87534122\n",
      " 0.92558809 0.87609512 0.85185185 0.85969616 0.83573201 0.90021548] \n",
      "Worst Group Accuracy: 0.41899441340782123\n",
      "Epoch 3 / 5\n",
      "Average training loss: 0.23295992945910493\n",
      "For Cross Val:\n",
      "Loss: 0.29892662167549133 Accuracy: 0.8642983621071271 \n",
      "Accuracy over subgroups: [0.68951049 0.70946822 0.51117318 0.61197917 0.55078125 0.6\n",
      " 0.52157598 0.61267606 0.84108706 0.8637037  0.86777344 0.82984531\n",
      " 0.92654825 0.85794743 0.88888889 0.82931189 0.8        0.88987267] \n",
      "Worst Group Accuracy: 0.5111731843575419\n",
      "Epoch 4 / 5\n",
      "Average training loss: 0.22489269615237945\n",
      "For Cross Val:\n",
      "Loss: 0.3192642331123352 Accuracy: 0.8565294378043382 \n",
      "Accuracy over subgroups: [0.71748252 0.72892348 0.51675978 0.61458333 0.5859375  0.8\n",
      " 0.53658537 0.64084507 0.8502994  0.85333333 0.85214844 0.8189263\n",
      " 0.91166587 0.81727159 0.85185185 0.79982127 0.77468983 0.88191969] \n",
      "Worst Group Accuracy: 0.5167597765363129\n",
      "Epoch 5 / 5\n",
      "Average training loss: 0.22006616591966904\n",
      "For Cross Val:\n",
      "Loss: 0.31137529015541077 Accuracy: 0.8615095174856131 \n",
      "Accuracy over subgroups: [0.6965035  0.692607   0.50558659 0.61197917 0.56445312 0.8\n",
      " 0.54409006 0.61971831 0.84246891 0.8654321  0.86699219 0.83621474\n",
      " 0.91694671 0.82728411 0.88888889 0.81233244 0.78560794 0.88818805] \n",
      "Worst Group Accuracy: 0.505586592178771\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "epochs = 5\n",
    "# loss_fn = ERMLoss(model, torch.nn.CrossEntropyLoss())\n",
    "loss_fn = GDROLoss(model, torch.nn.CrossEntropyLoss(), 0.01, 18, vector_subclass=True)\n",
    "\n",
    "train_epochs(epochs, tr, cv, test, model, loss_fn, optimizer, vector_subclass=True, verbose=True, num_subclasses=18, save_weights_name='CC_GDRO_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8594130750026162 \n",
      "Accuracy over subgroups: [0.69087608 0.6938326  0.55345395 0.64285714 0.59065765 0.5\n",
      " 0.56278465 0.65538736 0.84024115 0.85345683 0.86099161 0.8271028\n",
      " 0.90306586 0.81195145 0.82178218 0.81529235 0.78507776 0.88731248] \n",
      "Worst Group Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./epoch_5_CC_GDRO_2.wt'))\n",
    "_ = evaluate(test, model, 18, vector_subclass=True, replacement=False, get_loss=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n",
      "Average training loss: 0.16045779127432652\n",
      "For Cross Val:\n",
      "Loss: 0.24272045493125916 Accuracy: 0.9006197432492253 \n",
      "Accuracy over subgroups: [0.4993007  0.52658885 0.31843575 0.38802083 0.42773438 0.2\n",
      " 0.47842402 0.51995305 0.71856287 0.93382716 0.94023437 0.92902639\n",
      " 0.97839654 0.92803504 0.96296296 0.87042002 0.86451613 0.94225269] \n",
      "Worst Group Accuracy: 0.2\n",
      "Epoch 2 / 5\n",
      "Average training loss: 0.15031163605229683\n",
      "For Cross Val:\n",
      "Loss: 0.2443082481622696 Accuracy: 0.8950641876936698 \n",
      "Accuracy over subgroups: [0.58041958 0.57457847 0.48324022 0.43489583 0.45117188 0.2\n",
      " 0.53658537 0.55516432 0.73836942 0.92246914 0.92890625 0.86897179\n",
      " 0.97119539 0.92490613 0.96296296 0.84986595 0.84516129 0.93210578] \n",
      "Worst Group Accuracy: 0.2\n",
      "Epoch 3 / 5\n",
      "Average training loss: 0.1497923986340751\n",
      "For Cross Val:\n",
      "Loss: 0.23583410680294037 Accuracy: 0.900132802124834 \n",
      "Accuracy over subgroups: [0.55384615 0.53566796 0.49441341 0.39583333 0.4140625  0.2\n",
      " 0.49530957 0.53638498 0.71718102 0.92938272 0.93457031 0.87170155\n",
      " 0.97287566 0.93053817 0.96296296 0.87756926 0.85260546 0.94225269] \n",
      "Worst Group Accuracy: 0.2\n",
      "Epoch 4 / 5\n",
      "Average training loss: 0.14586693876767168\n",
      "For Cross Val:\n",
      "Loss: 0.2391911745071411 Accuracy: 0.897543160690571 \n",
      "Accuracy over subgroups: [0.57622378 0.57328145 0.49441341 0.421875   0.44140625 0.2\n",
      " 0.50281426 0.53638498 0.73883003 0.92395062 0.92851562 0.88989991\n",
      " 0.96759482 0.92803504 0.96296296 0.86595174 0.85856079 0.9358668 ] \n",
      "Worst Group Accuracy: 0.2\n",
      "Epoch 5 / 5\n",
      "Average training loss: 0.14224924239084355\n",
      "For Cross Val:\n",
      "Loss: 0.24043706059455872 Accuracy: 0.8968570163789288 \n",
      "Accuracy over subgroups: [0.62097902 0.62256809 0.54748603 0.48697917 0.49804688 0.2\n",
      " 0.55159475 0.56690141 0.75863657 0.91679012 0.92011719 0.87170155\n",
      " 0.96327412 0.91739675 0.92592593 0.84450402 0.84218362 0.93285015] \n",
      "Worst Group Accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "epochs = 5\n",
    "# loss_fn = ERMLoss(model, torch.nn.CrossEntropyLoss())\n",
    "loss_fn = UpweightLoss(model, torch.nn.CrossEntropyLoss(), 18, vector_subclass=True)\n",
    "\n",
    "train_epochs(epochs, tr, cv, test, model, loss_fn, optimizer, vector_subclass=True, verbose=True, num_subclasses=18, save_weights_name='CC_UW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8951129449402759 \n",
      "Accuracy over subgroups: [0.60689968 0.62070485 0.59292763 0.51031746 0.49170252 0.28571429\n",
      " 0.6044242  0.60240427 0.75146948 0.91589481 0.92079836 0.84080997\n",
      " 0.95587142 0.90196078 0.99009901 0.83358321 0.84658396 0.93230349] \n",
      "Worst Group Accuracy: 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate(test, model, 18, vector_subclass=True, replacement=False, get_loss=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n",
      "Average training loss: 0.2833691802580899\n",
      "For Cross Val:\n",
      "Loss: 0.3833000361919403 Accuracy: 0.83377600708278 \n",
      "Accuracy over subgroups: [0.6951049  0.68741894 0.61731844 0.57552083 0.65625    0.8\n",
      " 0.69418386 0.68309859 0.89958544 0.87209877 0.87851563 0.78616924\n",
      " 0.93086894 0.80600751 0.85185185 0.72832887 0.74640199 0.83428012] \n",
      "Worst Group Accuracy: 0.5755208333333334\n",
      "Epoch 2 / 5\n",
      "Average training loss: 0.2622008766604013\n",
      "For Cross Val:\n",
      "Loss: 0.3767387270927429 Accuracy: 0.834639220894201 \n",
      "Accuracy over subgroups: [0.71608392 0.73151751 0.67039106 0.60677083 0.66796875 0.6\n",
      " 0.69606004 0.67957746 0.89405804 0.85185185 0.85078125 0.74340309\n",
      " 0.92918867 0.78785982 0.85185185 0.71134942 0.74491315 0.84289912] \n",
      "Worst Group Accuracy: 0.6\n",
      "Epoch 3 / 5\n",
      "Average training loss: 0.2539220160999372\n",
      "For Cross Val:\n",
      "Loss: 0.2749040722846985 Accuracy: 0.8767817618415228 \n",
      "Accuracy over subgroups: [0.56923077 0.56031128 0.5027933  0.45572917 0.53710938 0.6\n",
      " 0.57410882 0.57159624 0.83049286 0.91975309 0.92832031 0.86715196\n",
      " 0.95895343 0.87984981 0.92592593 0.81054513 0.83622829 0.89653281] \n",
      "Worst Group Accuracy: 0.4557291666666667\n",
      "Epoch 4 / 5\n",
      "Average training loss: 0.24639365881484265\n",
      "For Cross Val:\n",
      "Loss: 0.2543395459651947 Accuracy: 0.8879814077025232 \n",
      "Accuracy over subgroups: [0.52447552 0.538262   0.45810056 0.390625   0.484375   0.6\n",
      " 0.50844278 0.51877934 0.79594657 0.93382716 0.93789062 0.8844404\n",
      " 0.96807489 0.90175219 0.92592593 0.85522788 0.86600496 0.91561214] \n",
      "Worst Group Accuracy: 0.390625\n",
      "Epoch 5 / 5\n",
      "Average training loss: 0.24177420116931095\n",
      "For Cross Val:\n",
      "Loss: 0.27710822224617004 Accuracy: 0.8797698096502877 \n",
      "Accuracy over subgroups: [0.57902098 0.59662776 0.53631285 0.42447917 0.51367188 0.6\n",
      " 0.55159475 0.56103286 0.82496545 0.92296296 0.92421875 0.84986351\n",
      " 0.96159386 0.89111389 0.92592593 0.83556747 0.84069479 0.90146915] \n",
      "Worst Group Accuracy: 0.4244791666666667\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "epochs = 5\n",
    "# loss_fn = ERMLoss(model, torch.nn.CrossEntropyLoss())\n",
    "loss_fn = ERMGDROLoss(model, torch.nn.CrossEntropyLoss(), 0.01, 18, t=0.5, vector_subclass=True)\n",
    "\n",
    "train_epochs(epochs, tr, cv, test, model, loss_fn, optimizer, vector_subclass=True, verbose=True, num_subclasses=18, save_weights_name='CC_ERM_GDRO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8798941561645064 \n",
      "Accuracy over subgroups: [0.57966409 0.5969163  0.59210526 0.5015873  0.52427781 0.35714286\n",
      " 0.5862069  0.58637578 0.82351168 0.91978167 0.92030468 0.8258567\n",
      " 0.95322701 0.88272642 0.97029703 0.83568216 0.84169142 0.9035082 ] \n",
      "Worst Group Accuracy: 0.35714285714285715\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate(test, model, 18, vector_subclass=True, replacement=False, get_loss=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Method that better approximates WILDS\n",
    "\n",
    "Gradient Clip (Norm): 1\n",
    "AdamW\n",
    "linear_schedule_with_warmup (warmup): 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertClassifier(device=DEVICE)\n",
    "epochs = 5\n",
    "loss_fn = ERMLoss(model, torch.nn.CrossEntropyLoss())\n",
    "\n",
    "num_training_steps = ceil(len(tr.dataset) / 16) * epochs\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001, weight_decay=0.01)\n",
    "\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "#note scheduler is in wrong place in train_eval currently\n",
    "\n",
    "train_epochs(epochs, tr, cv, test, model, loss_fn, optimizer, scheduler=scheduler, vector_subclass=True, verbose=True, num_subclasses=18, save_weights_name='CC_ERM_GDRO', gradient_clip = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate(test, model, 18, vector_subclass=True, replacement=False, get_loss=False, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPUiuP8oFwoTkVWCrrQzyn9",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "civil_comments_test.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "REU",
   "language": "python",
   "name": "reu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
